{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Q&A App with PyTorch\n",
    "\n",
    "In this notebook, we will build a Question and Answer (Q&A) application using PyTorch. The Q&A application will take a given question and a context paragraph as input, and it will provide the answer to the question based on the information in the context paragraph.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using the SQuAD (Stanford Question Answering Dataset) dataset for training and evaluating our Q&A model. The SQuAD dataset consists of questions and answers pairs, where each question is associated with a context paragraph. Our goal is to train a model that can accurately answer questions based on the given context. Dataset is available at https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "\n",
    "To load and read data from a JSON file in Python, you can use the `json` module. Here's an example of how to do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train-v2.0.json\", 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed structure should be similar to the one provided below. From this data we will focus on the `question` and `answers` fields where the topic is `Premier League`. This will provide us with exact answers to a specific number of questions.\n",
    "\n",
    "To obtain the `questions` and `answers`, define and run the following function `get_qa`. This should return a set of 357 pairs of questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available questions: 357\n"
     ]
    }
   ],
   "source": [
    "# get the available questions and answers for a given topic\n",
    "def get_qa(topic, data):\n",
    "    q = []\n",
    "    a = []\n",
    "    for d in data['data']:\n",
    "        if d['title']==topic:\n",
    "            for paragraph in d['paragraphs']:\n",
    "                for qa in paragraph['qas']:\n",
    "                    if not qa['is_impossible']:\n",
    "                        q.append(qa['question'])\n",
    "                        a.append(qa['answers'][0]['text'])\n",
    "            return q,a\n",
    "\n",
    "questions, answers = get_qa(topic='Premier_League', data=data)\n",
    "\n",
    "print(\"Number of available questions: {}\".format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Embedding Model\n",
    "\n",
    "We will be using a pre-trained transformer-based model for our Q&A application. QA embedding model transforms QA into a format that a computer can understand. It converts text to numbers, then compare those numbers to find the best match, and use that match to provide the user with an answer.\n",
    "\n",
    "Training a model from scratch is time consuming and expensive. This is where HuggingFace comes in. The shell script for downloading the pretrained model from HuggingFace is provided. If you are using Windows, use ‘curl’.\n",
    "\n",
    "~~~\n",
    "#!/bin/bash\n",
    "# defines the qa model\n",
    "MODEL_DIR=\"https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main\"\n",
    "MODEL_NAME=\"paraphrase-MiniLM-L6-v2\"\n",
    "# downloads the qa model. To make this image more general one can use curl\n",
    "# with the \"-O\" argument to download the necessary files defined\n",
    "# in \"require.txt\".\n",
    "mkdir ${MODEL_NAME} &&\\\n",
    "    curl -o ${MODEL_NAME}/vocab.txt ${MODEL_DIR}/vocab.txt &&\\\n",
    "    curl -o ${MODEL_NAME}/tokenizer_config.json ${MODEL_DIR}/tokenizer_config.json &&\\\n",
    "    curl -o ${MODEL_NAME}/tokenizer.json ${MODEL_DIR}/tokenizer.json &&\\\n",
    "    curl -o ${MODEL_NAME}/special_tokens_map.json ${MODEL_DIR}/special_tokens_map.json &&\\\n",
    "    curl -o ${MODEL_NAME}/sentence_bert_config.json ${MODEL_DIR}/sentence_bert_config.json &&\\\n",
    "    curl -o ${MODEL_NAME}/pytorch_model.bin ${MODEL_DIR}/pytorch_model.bin &&\\\n",
    "    curl -o ${MODEL_NAME}/modules.json ${MODEL_DIR}/modules.json &&\\\n",
    "    curl -o ${MODEL_NAME}/config_sentence_transformers.json ${MODEL_DIR}/config_sentence_transformers.json &&\\\n",
    "    curl -o ${MODEL_NAME}/config.json ${MODEL_DIR}/config.json\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t have the `transformers` package, start by installing it with pip.\n",
    "\n",
    "'pip install transformers'\n",
    "\n",
    "Then, run the following `get_model` function. If all files were downloaded properly and all dependencies met, this should run without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "def get_model(model_name):\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "  \n",
    "model, tokenizer = get_model(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now run our embedding model over a sample of the context questions. To do this, run the following instructions.\n",
    "\n",
    "The code should print the shape of our new embeddings vector.\n",
    "\n",
    "`Embeddings shape: torch.Size([3, 384]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 384])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "# source: https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    \n",
    "    input_mask_expanded = (\n",
    "      attention_mask\n",
    "      .unsqueeze(-1)\n",
    "      .expand(token_embeddings.size())\n",
    "      .float()\n",
    "    )\n",
    "    \n",
    "    pool_emb = (\n",
    "      torch.sum(token_embeddings * input_mask_expanded, 1) \n",
    "      / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    )\n",
    "    \n",
    "    return pool_emb\n",
    "\n",
    "def get_embeddings(questions, tokenizer, model):\n",
    "  # Tokenize sentences\n",
    "  encoded_input = tokenizer(questions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "  # Compute token embeddings\n",
    "  with torch.no_grad():\n",
    "      model_output = model(**encoded_input)\n",
    "\n",
    "  # Average pooling\n",
    "  embeddings = mean_pooling(model_output, encoded_input['attention_mask']) \n",
    "  \n",
    "  return embeddings\n",
    "\n",
    "embeddings = get_embeddings(questions[:3], tokenizer, model)\n",
    "print(\"Embeddings shape: {}\".format(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by checking our previous sample questions:\n",
    "\n",
    "~~~\n",
    "[\n",
    "  'How many club members are there?',\n",
    "  'How many matches does each team play?',\n",
    "  'What days are most games played?'\n",
    "]\n",
    "~~~\n",
    "\n",
    "Then, paraphrase the last one to:\n",
    "\n",
    "'Which days have the most events played at?'\n",
    "\n",
    "Finally, let’s embed our new question and calculate the Euclidean distance between `new_embedding` and `embeddings`.\n",
    "\n",
    "The code should output the following distances, indicating that the last question in our sample is indeed the closest (smallest distance) to our new question.\n",
    "\n",
    "`tensor([71.4029, 59.8726, 23.9430])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([71.4030, 59.8726, 23.9431])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_question = 'Which days have the most events played at?'\n",
    "new_embedding = get_embeddings([new_question], tokenizer, model)\n",
    "\n",
    "# squared Euclidean distance between sample questions and new_question\n",
    "((embeddings - new_embedding)**2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAEmbedder:\n",
    "  def __init__(self, model_name=\"paraphrase-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Defines a QA embedding model. This is, given a set of questions,\n",
    "    this class returns the corresponding embedding vectors.\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    self.model = None\n",
    "    self.tokenizer = None\n",
    "    self.model_name = model_name\n",
    "    self.set_model(model_name)\n",
    "  \n",
    "  \n",
    "  def get_model(self, model_name):\n",
    "    \"\"\"\n",
    "    Loads a general tokenizer and model using pytorch\n",
    "    'AutoTokenizer' and 'AutoModel'\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "  \n",
    "  \n",
    "  def set_model(self, model_name):\n",
    "    \"\"\"\n",
    "    Sets a general tokenizer and model using the 'self.get_model'\n",
    "    method.\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    self.model, self.tokenizer = self.get_model(self.model_name)\n",
    "  \n",
    "  \n",
    "  def _mean_pooling(self, model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Internal method that takes a model output and an attention\n",
    "    mask and outputs a mean pooling layer.\n",
    "    \n",
    "    Args:\n",
    "      model_output (`torch.Tensor`): output from the QA model\n",
    "      attention_mask (`torch.Tensor`): attention mask defined in the QA tokenizer\n",
    "      \n",
    "    Returns:\n",
    "      The averaged tensor.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output[0]\n",
    "    \n",
    "    input_mask_expanded = (\n",
    "      attention_mask\n",
    "      .unsqueeze(-1)\n",
    "      .expand(token_embeddings.size())\n",
    "      .float()\n",
    "    )\n",
    "    \n",
    "    pool_emb = (\n",
    "      torch.sum(token_embeddings * input_mask_expanded, 1) \n",
    "      / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    )\n",
    "    \n",
    "    return pool_emb\n",
    "  \n",
    "  \n",
    "  def get_embeddings(self, questions, batch=32):\n",
    "    \"\"\"\n",
    "    Gets the corresponding embeddings for a set of input 'questions'.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`): List of strings defining the questions to be embedded\n",
    "      batch (`int`): Performs the embedding job 'batch' questions at a time\n",
    "      \n",
    "    Returns:\n",
    "      The embedding vectors.\n",
    "    \"\"\"\n",
    "    question_embeddings = []\n",
    "    for i in range(0, len(questions), batch):\n",
    "    \n",
    "        # Tokenize sentences\n",
    "        encoded_input = self.tokenizer(questions[i:i+batch], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        batch_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        question_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    question_embeddings = torch.cat(question_embeddings, dim=0)\n",
    "    return question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QASearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QASearcher:\n",
    "  def __init__(self, model_name=\"paraphrase-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Defines a QA Search model. This is, given a new question it searches\n",
    "    the most similar questions in a set 'context' and returns both the best\n",
    "    question and associated answer.\n",
    "    \n",
    "    Args:\n",
    "      model_name (`str`): Directory containing the necessary tokenizer\n",
    "        and model files.\n",
    "    \"\"\"\n",
    "    self.answers = None\n",
    "    self.questions = None\n",
    "    self.question_embeddings = None\n",
    "    self.embedder = QAEmbedder(model_name=model_name)\n",
    "  \n",
    "  \n",
    "  def set_context_qa(self, questions, answers):\n",
    "    \"\"\"\n",
    "    Sets the QA context to be used during search.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "      answers (`list` of `str`): Best answer for each question in 'questions'\n",
    "    \"\"\"\n",
    "    self.answers = answers\n",
    "    self.questions = questions\n",
    "    self.question_embeddings = self.get_q_embeddings(questions)\n",
    "  \n",
    "  \n",
    "  def get_q_embeddings(self, questions):\n",
    "    \"\"\"\n",
    "    Gets the embeddings for the questions in 'context'.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "    \n",
    "    Returns:\n",
    "      The embedding vectors.\n",
    "    \"\"\"\n",
    "    question_embeddings = self.embedder.get_embeddings(questions)\n",
    "    question_embeddings  = torch.nn.functional.normalize(question_embeddings, p=2, dim=1)\n",
    "    return question_embeddings.transpose(0,1)\n",
    "  \n",
    "  \n",
    "  def cosine_similarity(self, questions, batch=32):\n",
    "    \"\"\"\n",
    "    Gets the cosine similarity between the new questions and the 'context' questions.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "      batch (`int`): Performs the embedding job 'batch' questions at a time\n",
    "    \n",
    "    Returns:\n",
    "      The cosine similarity\n",
    "    \"\"\"\n",
    "    question_embeddings = self.embedder.get_embeddings(questions, batch=batch)\n",
    "    question_embeddings = torch.nn.functional.normalize(question_embeddings, p=2, dim=1)\n",
    "    \n",
    "    cosine_sim = torch.mm(question_embeddings, self.question_embeddings)\n",
    "    \n",
    "    return cosine_sim\n",
    "  \n",
    "  \n",
    "  def get_answers(self, questions, batch=32):\n",
    "    \"\"\"\n",
    "    Gets the best answers in the stored 'context' for the given new 'questions'.\n",
    "    \n",
    "    Args:\n",
    "      questions (`list` of `str`):  List of strings defining the questions to be embedded\n",
    "      batch (`int`): Performs the embedding job 'batch' questions at a time\n",
    "    \n",
    "    Returns:\n",
    "      A `list` of `dict`'s containing the original question ('orig_q'), the most similar\n",
    "      question in the context ('best_q') and the associated answer ('best_a').\n",
    "    \"\"\"\n",
    "    similarity = self.cosine_similarity(questions, batch=batch)\n",
    "    \n",
    "    response = []\n",
    "    for i in range(similarity.shape[0]):\n",
    "      best_ix = similarity[i].argmax()\n",
    "      best_q = self.questions[best_ix]\n",
    "      best_a = self.answers[best_ix]\n",
    "      \n",
    "      response.append(\n",
    "        {\n",
    "          'orig_q':questions[i],\n",
    "          'best_q':best_q,\n",
    "          'best_a':best_a,\n",
    "        }\n",
    "      )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the FastAPI app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (0.108.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from fastapi) (4.9.0)\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from fastapi) (0.32.0.post1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from fastapi) (2.5.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.14.6)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from starlette<0.33.0,>=0.29.0->fastapi) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\victo\\Git\\vbcalinao\\boomai-mle\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uvicorn in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from uvicorn) (4.9.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\victo\\git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\victo\\Git\\vbcalinao\\boomai-mle\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\victo\\Git\\vbcalinao\\boomai-mle\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastapi\n",
    "%pip install uvicorn\n",
    "%pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QASearcher' from 'utils' (c:\\Users\\victo\\Git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muvicorn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, Request\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QASearcher\n\u001b[0;32m      5\u001b[0m app \u001b[38;5;241m=\u001b[39m FastAPI()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/set_context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_context\u001b[39m(data:Request):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'QASearcher' from 'utils' (c:\\Users\\victo\\Git\\vbcalinao\\boomai-mle\\venv\\lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from app.utils import QASearcher\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/set_context\")\n",
    "async def set_context(data:Request):\n",
    "  \"\"\"\n",
    "  Fastapi POST method that sets the QA context for search.\n",
    "  \n",
    "  Args:\n",
    "    data(`dict`): Two fields required 'questions' (`list` of `str`)\n",
    "      and 'answers' (`list` of `str`)\n",
    "  \"\"\"\n",
    "  data = await data.json()\n",
    "  \n",
    "  qa_search.set_context_qa(\n",
    "    data['questions'], \n",
    "    data['answers']\n",
    "  )\n",
    "  return {\"message\": \"Search context set\"}\n",
    "\n",
    "\n",
    "@app.post(\"/get_answer\")\n",
    "async def get_answer(data:Request):\n",
    "  \"\"\"\n",
    "  Fastapi POST method that gets the best question and answer \n",
    "  in the set context.\n",
    "  \n",
    "  Args:\n",
    "    data(`dict`): One field required 'questions' (`list` of `str`)\n",
    "  \n",
    "  Returns:\n",
    "    A `dict` containing the original question ('orig_q'), the most similar\n",
    "    question in the context ('best_q') and the associated answer ('best_a').\n",
    "  \"\"\"\n",
    "  data = await data.json()\n",
    "  \n",
    "  response = qa_search.get_answers(data['questions'], batch=1)\n",
    "  return response\n",
    "\n",
    "\n",
    "# initialises the QA model and starts the uvicorn app\n",
    "if __name__ == \"__main__\":\n",
    "  qa_search = QASearcher()\n",
    "  uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
